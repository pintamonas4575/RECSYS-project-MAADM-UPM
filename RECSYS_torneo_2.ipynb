{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8eeafe4",
   "metadata": {},
   "source": [
    "# **Filtrado basado en contenido**\n",
    "\n",
    "Álvaro Fraile, Jaime Álvarez, Alejandro Mendoza\n",
    "\n",
    "https://www.kaggle.com/competitions/recsys-filtrado-basado-en-contenido-2425"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e441b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f10d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\workspace\\RECSYS-project-MAADM-UPM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1edd6f",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b92c7",
   "metadata": {},
   "source": [
    "### Negocios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27bef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negocios initial memory usage: 31.01 MB\n",
      "Negocios final memory usage: 19.24 MB\n",
      "***** Preprocesamiento negocios: 0 minutos y 0 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "negocios_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/negocios.csv')\n",
    "initial_memory = negocios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "negocios_df.drop(columns=['address', 'postal_code', 'is_open', 'hours'], inplace=True)\n",
    "for col in negocios_df.select_dtypes(include=['object']):\n",
    "    negocios_df[col] = negocios_df[col].astype(\"category\")\n",
    "negocios_df['latitude'] = negocios_df['latitude'].astype('float16')\n",
    "negocios_df['longitude'] = negocios_df['longitude'].astype('float16')\n",
    "negocios_df['stars'] = negocios_df['stars'].astype('float16')\n",
    "negocios_df['review_count'] = negocios_df['review_count'].astype('int16')\n",
    "\n",
    "final_memory = negocios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento negocios: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8697ab",
   "metadata": {},
   "source": [
    "### Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7456d25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_9628\\57253039.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  usuarios_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/usuarios.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negocios initial memory usage: 1265.39 MB\n",
      "Negocios final memory usage: 1114.46 MB\n",
      "***** Preprocesamiento usuarios: 0 minutos y 9 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "usuarios_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/usuarios.csv')\n",
    "initial_memory = usuarios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "usuarios_df.drop(columns=['elite', 'yelping_since'], inplace=True)\n",
    "usuarios_df['user_id'] = usuarios_df['user_id'].astype('string')\n",
    "usuarios_df['name'] = usuarios_df['name'].astype('category')\n",
    "usuarios_df['friends'] = usuarios_df['friends'].astype('category')\n",
    "usuarios_df['useful'] = usuarios_df['useful'].astype('int32')\n",
    "usuarios_df['funny'] = usuarios_df['funny'].astype('int32')\n",
    "usuarios_df['cool'] = usuarios_df['cool'].astype('int32')\n",
    "usuarios_df['average_stars'] = usuarios_df['average_stars'].astype('float16')\n",
    "for col in usuarios_df.select_dtypes(include=['int64']):\n",
    "    usuarios_df[col] = usuarios_df[col].astype('uint16')\n",
    "\n",
    "final_memory = usuarios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento usuarios: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ecacb",
   "metadata": {},
   "source": [
    "### Train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e23a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews initial memory usage: 859.81 MB\n",
      "Train reviews final memory usage: 727.39 MB\n",
      "***** Preprocesamiento train reviews: 0 minutos y 7 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_reviews_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/train_reviews.csv')\n",
    "initial_memory = train_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "train_reviews_df.drop(columns=['date'], inplace=True)\n",
    "train_reviews_df['review_id'] = train_reviews_df['review_id'].astype('string')\n",
    "train_reviews_df['user_id'] = train_reviews_df['user_id'].astype('category')\n",
    "train_reviews_df['business_id'] = train_reviews_df['business_id'].astype('category')\n",
    "train_reviews_df['text'] = train_reviews_df['text'].astype('string')\n",
    "\n",
    "final_memory = train_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento train reviews: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e232282",
   "metadata": {},
   "source": [
    "### Test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddbb3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews initial memory usage: 365.37 MB\n",
      "Train reviews final memory usage: 314.81 MB\n",
      "***** Preprocesamiento test reviews: 0 minutos y 3 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "test_reviews_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/test_reviews.csv')\n",
    "initial_memory = test_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "test_reviews_df.drop(columns=['date'], inplace=True)\n",
    "test_reviews_df['review_id'] = test_reviews_df['review_id'].astype('string')\n",
    "test_reviews_df['user_id'] = test_reviews_df['user_id'].astype('category')\n",
    "test_reviews_df['business_id'] = test_reviews_df['business_id'].astype('category')\n",
    "test_reviews_df['text'] = test_reviews_df['text'].astype('string')\n",
    "\n",
    "final_memory = test_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento test reviews: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba6830",
   "metadata": {},
   "source": [
    "## **Submission DataFrame skeleton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bfafd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = test_reviews_df[['review_id']].copy() # dataframe with review_id column\n",
    "global_avg = train_reviews_df['stars'].mean() # global average rating value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb7024",
   "metadata": {},
   "source": [
    "## Aproximación 1 - Media del negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb307f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of avg_ratings: 30064\n",
      "Length of negocios_df: 30069\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>avg_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--ARBQr1WMsTWiwOKOj-FQ</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--LC8cIrALInl2vyo701tg</td>\n",
       "      <td>4.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--N9yp3ZWqQIm7DqKRvorg</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--S43ruInmIsGrnnkmavRw</td>\n",
       "      <td>3.380952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  avg_stars\n",
       "0  --7PUidqRWpRSpXebiyxTg   1.900000\n",
       "1  --ARBQr1WMsTWiwOKOj-FQ   4.666667\n",
       "2  --LC8cIrALInl2vyo701tg   4.600000\n",
       "3  --N9yp3ZWqQIm7DqKRvorg   2.500000\n",
       "4  --S43ruInmIsGrnnkmavRw   3.380952"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average rating for each business\n",
    "avg_ratings = train_reviews_df.groupby('business_id', observed=True)['stars'].mean().reset_index()\n",
    "avg_ratings.columns = ['business_id', 'avg_stars']\n",
    "print(\"Length of avg_ratings:\", len(avg_ratings))\n",
    "print(\"Length of negocios_df:\", len(negocios_df))\n",
    "avg_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e96194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [01:19<00:00, 5246.84it/s]\n"
     ]
    }
   ],
   "source": [
    "output_df_1 = output_df.copy()\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df_1.loc[index, 'stars'] = (\n",
    "        avg_ratings.loc[avg_ratings['business_id'] == review['business_id'], 'avg_stars'].values[0]\n",
    "        if review['business_id'] in avg_ratings['business_id'].values else global_avg\n",
    "    )\n",
    "\n",
    "output_df_1.to_csv('results_tournament_2/submission_business_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd2bc3",
   "metadata": {},
   "source": [
    "MAE Público obtenido: \n",
    "* Usando 3 como default: **1.0433**\n",
    "* Usando media global como default: **1.0433**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24a2b4",
   "metadata": {},
   "source": [
    "## Aproximación 1.1 - Con redondeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780e60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_2 = output_df_1.copy() \n",
    "output_df_2['stars'] = output_df_2['stars'].round()\n",
    "output_df_2.to_csv('results_tournament_2/submission_business_avg_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96f8b2",
   "metadata": {},
   "source": [
    "MAE Público obtenido con redondeo: **1.0286**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0788d1",
   "metadata": {},
   "source": [
    "## Aproximación 2 - Embeddings con TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee5e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizando con TF-IDF...\n",
      "***** TF-IDF vectorization: 0 minutes 34 seconds *****\n",
      "Calculando similitud...\n",
      "***** Cosine similarity: 2 minutes 24 seconds *****\n",
      "Creando diccionario de ratings por usuario...\n",
      "***** User ratings dictionary: 4 minutes 5 seconds *****\n",
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:54<00:00, 7542.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Total time: 8 minutes 0 seconds *****\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "# Paso 1: Agrupar reviews por negocio\n",
    "business_reviews = train_reviews_df.groupby('business_id', observed=True)['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Paso 2: Vectorizar con TF-IDF\n",
    "print(\"Vectorizando con TF-IDF...\")\n",
    "start = time.time()\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(business_reviews['text'])\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** TF-IDF vectorization: {int(minutos)} minutes {int(segundos)} seconds *****\")\n",
    "\n",
    "# Paso 3: Calcular similitud entre negocios\n",
    "print(\"Calculando similitud...\")\n",
    "start = time.time()\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Cosine similarity: {int(minutos)} minutes {int(segundos)} seconds *****\")\n",
    "\n",
    "# Paso 4: Índice para acceder por business_id\n",
    "business_indices = pd.Series(business_reviews.index, index=business_reviews['business_id'])\n",
    "\n",
    "# Paso 5: Crear un diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "start = time.time()\n",
    "user_ratings = train_reviews_df.groupby('user_id', observed=True)\n",
    "user_ratings = user_ratings.apply(lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars'], include_groups=False).to_dict()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** User ratings dictionary: {int(minutos)} minutes {int(segundos)} seconds *****\")\n",
    "\n",
    "# Paso 6: Función para predecir rating\n",
    "def predict_rating(user_id: str, target_business_id: str) -> float:\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "\n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0:  # Considerar solo similares positivos\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg # No hay similitud con los ítems que ha valorado\n",
    "\n",
    "    # Promedio ponderado\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 7: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "output_df_3 = output_df.copy()\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df_3.loc[index, 'stars'] = predict_rating(review['user_id'], review['business_id'])\n",
    "\n",
    "total_minutes, total_seconds = divmod(time.time() - total_time, 60)\n",
    "print(f\"***** Total time: {int(total_minutes)} minutes {int(total_seconds)} seconds *****\")\n",
    "\n",
    "output_df_3['stars'] = output_df_3['stars'].round()\n",
    "output_df_3.to_csv('results_tournament_2/submission_tfidf_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00896ad5",
   "metadata": {},
   "source": [
    "MAE público obtenido con TFIDF: **1.1597**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e9e0e",
   "metadata": {},
   "source": [
    "## Aproximación 3 - Embeddings con Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86aaad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Cargando modelo de sentence-transformers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Carga de modelo: 0 minutos 7 segundos *****\n",
      "Vectorizando con sentence-transformers...\n",
      "***** Vectorización: 1 minutos 23 segundos *****\n",
      "Calculando similitud...\n",
      "***** Cosine similarity: 0 minutos 1 segundos *****\n",
      "Creando diccionario de ratings por usuario...\n",
      "***** User ratings dictionary: 4 minutos 14 segundos *****\n",
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:57<00:00, 7207.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Total time: 6 minutes 45 seconds *****\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "# Paso 1: Agrupar reviews por negocio\n",
    "business_reviews = train_reviews_df.groupby('business_id', observed=True)['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "print(\"Cargando modelo de sentence-transformers...\")\n",
    "start = time.time()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Carga de modelo: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 2: Vectorizar con SentenceTransformer en GPU\n",
    "print(\"Vectorizando con sentence-transformers...\")\n",
    "start = time.time()\n",
    "embeddings = model.encode(business_reviews['text'].tolist(), convert_to_tensor=True, device=device)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Vectorización: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 3: Calcular similitud entre negocios\n",
    "print(\"Calculando similitud...\")\n",
    "start = time.time()\n",
    "cosine_sim = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()  # Para usarlo como matriz normal\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Cosine similarity: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 4: Índice para acceder por business_id\n",
    "business_indices = pd.Series(business_reviews.index, index=business_reviews['business_id'])\n",
    "\n",
    "# Paso 5: Crear un diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "start = time.time()\n",
    "user_ratings = train_reviews_df.groupby('user_id', observed=True)\n",
    "user_ratings = user_ratings.apply(lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars'], include_groups=False).to_dict()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** User ratings dictionary: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 6: Función para predecir rating\n",
    "def predict_rating(user_id: str, target_business_id: str) -> float:\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0:  # Considerar solo similares positivos\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg  # No hay similitud con los ítems que ha valorado\n",
    "\n",
    "    # Promedio ponderado\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 7: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "output_df_4 = output_df.copy()\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df_4.loc[index, 'stars'] = predict_rating(review['user_id'], review['business_id'])\n",
    "\n",
    "total_minutes, total_seconds = divmod(time.time() - total_time, 60)\n",
    "print(f\"***** Total time: {int(total_minutes)} minutes {int(total_seconds)} seconds *****\")\n",
    "\n",
    "output_df_4['stars'] = output_df_4['stars'].round()\n",
    "output_df_4.to_csv('results_tournament_2/submission_sentence_transformers_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82faf36f",
   "metadata": {},
   "source": [
    "MAE público obtenido: **1.1594**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceefe61",
   "metadata": {},
   "source": [
    "## Aproximación 3.1 - Embeddings con Sentence Transformers de reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec5cc7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Cargando modelo de sentence-transformers...\n",
      "***** Carga de modelo: 0 minutos 1 segundos *****\n",
      "Vectorizando cada review individual...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1891/1891 [05:26<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Vectorización: 5 minutos 41 segundos *****\n",
      "Calculando embedding medio por negocio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30064/30064 [00:04<00:00, 7123.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de embeddings...\n",
      "***** Creación de matriz de embeddings: 0 minutos 0 segundos *****\n",
      "Calculando similitud coseno...\n",
      "***** Cálculo de similitud coseno: 0 minutos 1 segundos *****\n",
      "Creando diccionario de ratings por usuario...\n",
      "***** User ratings dictionary: 4 minutos 17 segundos *****\n",
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:49<00:00, 8436.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Total time: 11 minutes 10 seconds *****\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "print(\"Cargando modelo de sentence-transformers...\")\n",
    "start = time.time()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Carga de modelo: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 1: Vectorizar cada review individual\n",
    "print(\"Vectorizando cada review individual...\")\n",
    "start = time.time()\n",
    "review_embeddings = model.encode(\n",
    "    train_reviews_df['text'].tolist(),\n",
    "    convert_to_tensor=True,\n",
    "    device=device,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=512\n",
    ")\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Vectorización: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 2: Asociar embeddings a cada review\n",
    "train_reviews_df['embedding'] = list(review_embeddings.cpu())\n",
    "\n",
    "# Paso 3: Agrupar por negocio y sacar el embedding medio\n",
    "print(\"Calculando embedding medio por negocio...\")\n",
    "business_embeddings = {}\n",
    "for business_id, group in tqdm(train_reviews_df.groupby('business_id', observed=True)):\n",
    "    embs = torch.stack(group['embedding'].tolist())\n",
    "    business_embeddings[business_id] = embs.mean(dim=0)\n",
    "\n",
    "# Paso 4: Crear lista de embeddings en orden\n",
    "print(\"Creando matriz de embeddings...\")\n",
    "start = time.time()\n",
    "business_ids = list(business_embeddings.keys())\n",
    "embedding_matrix = torch.stack([business_embeddings[b_id] for b_id in business_ids])\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Creación de matriz de embeddings: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 5: Calcular similitud coseno entre negocios\n",
    "print(\"Calculando similitud coseno...\")\n",
    "start = time.time()\n",
    "cosine_sim = util.pytorch_cos_sim(embedding_matrix, embedding_matrix).cpu().numpy()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Cálculo de similitud coseno: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 6: Índice para acceder por business_id\n",
    "business_indices = pd.Series(range(len(business_ids)), index=business_ids)\n",
    "\n",
    "# Paso 7: Crear diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "start = time.time()\n",
    "user_ratings = train_reviews_df.groupby('user_id', observed=True)\n",
    "user_ratings = user_ratings.apply(lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars'], include_groups=False).to_dict()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** User ratings dictionary: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 8: Función para predecir rating\n",
    "def predict_rating(user_id, target_business_id):\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "    \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0.8:\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg\n",
    "\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 9: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "output_df_5 = output_df.copy()\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df_5.loc[index, 'stars'] = predict_rating(review['user_id'], review['business_id'])\n",
    "\n",
    "total_minutes, total_seconds = divmod(time.time() - total_time, 60)\n",
    "print(f\"***** Total time: {int(total_minutes)} minutes {int(total_seconds)} seconds *****\")\n",
    "\n",
    "# Paso 10: Limpiar NaNs y redondear\n",
    "output_df_5['stars'] = output_df_5['stars'].fillna(global_avg).round()\n",
    "output_df_5.to_csv('results_tournament_2/submission_sentence_transformers_rounded_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c2041d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999998  0.4427101  0.29980582 ... 0.6780671  0.6563667  0.59870136]\n",
      " [0.4427101  0.99999994 0.44541568 ... 0.24072587 0.3650766  0.3196715 ]\n",
      " [0.29980582 0.44541568 0.99999994 ... 0.16819778 0.259027   0.21196762]\n",
      " ...\n",
      " [0.6780671  0.24072587 0.16819778 ... 1.         0.5487293  0.61004245]\n",
      " [0.6563667  0.3650766  0.259027   ... 0.5487293  0.9999999  0.5282265 ]\n",
      " [0.59870136 0.3196715  0.21196762 ... 0.61004245 0.5282265  0.9999999 ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d98a61",
   "metadata": {},
   "source": [
    "MAE público: **1.1614**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bdb2f",
   "metadata": {},
   "source": [
    "### \"Deprecated\" cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b519ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:45<00:00, 9182.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Paso 8: Función para predecir rating\n",
    "def predict_rating(user_id, target_business_id):\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "    \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0.8:\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg\n",
    "\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 9: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df.loc[index, 'stars'] = predict_rating(review.user_id, review.business_id)\n",
    "\n",
    "# Paso 10: Limpiar NaNs y redondear\n",
    "output_df['stars'] = output_df['stars'].fillna(global_avg).round()\n",
    "output_df.to_csv('./results/submission_sentence_transformers_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando distancia euclidiana...\n",
      "Creando diccionario de ratings por usuario...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrai\\AppData\\Local\\Temp\\ipykernel_10640\\1767733142.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_ratings = train_reviews_df.groupby('user_id').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:45<00:00, 9091.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Euclidean distance instead of cosine similarity\n",
    "print(\"Calculando distancia euclidiana...\")\n",
    "euclidean_dist = torch.cdist(embedding_matrix, embedding_matrix, p=2).cpu().numpy()\n",
    "\n",
    "# Convert Euclidean distance to similarity (1 / (1 + distance))\n",
    "euclidean_sim = 1 / (1 + euclidean_dist)\n",
    "\n",
    "# Paso 6: Índice para acceder por business_id\n",
    "business_indices = pd.Series(range(len(business_ids)), index=business_ids)\n",
    "\n",
    "# Paso 7: Crear diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "user_ratings = train_reviews_df.groupby('user_id').apply(\n",
    "    lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars']\n",
    ").to_dict()\n",
    "\n",
    "# Paso 8: Función para predecir rating\n",
    "def predict_rating(user_id, target_business_id):\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "    \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = euclidean_sim[target_idx, rated_idx]\n",
    "            if sim > 0.8:\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg\n",
    "\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc295474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:44<00:00, 9239.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Paso 9: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df.loc[index, 'stars'] = predict_rating(review.user_id, review.business_id)\n",
    "\n",
    "# Paso 10: Limpiar NaNs y redondear\n",
    "output_df['stars'] = output_df['stars'].fillna(global_avg).round()\n",
    "output_df.to_csv('./results/submission_sentence_transformers_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18c5da",
   "metadata": {},
   "source": [
    "## Aproximación 4 - Analisis de sentimientos (ejecutar en kernel nuevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28174eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Carga de modelo: 0 minutos 0 segundos *****\n",
      "***** Preprocesamiento textos: 0 minutos 0 segundos *****\n",
      "Calculando sentimiento por batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando batches: 100%|██████████| 1621/1621 [07:46<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cálculo de scores: 7 minutos 46 segundos *****\n",
      "***** Total time: 7 minutos 47 segundos *****\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "start = time.time()\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.eval()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Carga de modelo: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Preprocesar textos\n",
    "MAX_LEN = 256\n",
    "start = time.time()\n",
    "texts = test_reviews_df['text'].copy().apply(lambda x: x[:MAX_LEN]).tolist()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento textos: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Función para procesar en lotes\n",
    "def get_sentiment_scores_batched(texts: list, batch_size=256) -> list:\n",
    "    scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Procesando batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        encoded_batch = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "        encoded_batch = {k: v.to(device) for k, v in encoded_batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_batch)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            probs = softmax(logits, axis=1)\n",
    "\n",
    "            # Map: negative → 0, neutral → 2.5, positive → 5\n",
    "            batch_scores = probs[:, 0]*0 + probs[:, 1]*2.5 + probs[:, 2]*5\n",
    "            scores.extend(batch_scores.tolist())\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Calcular los scores en batch\n",
    "print(\"Calculando sentimiento por batches...\")\n",
    "start = time.time()\n",
    "sentiment_scores = get_sentiment_scores_batched(texts, batch_size=MAX_LEN)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Cálculo de scores: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time.time() - total_time, 60)\n",
    "print(f\"***** Total time: {int(total_minutes)} minutos {int(total_seconds)} segundos *****\")\n",
    "\n",
    "output_df_6 = test_reviews_df[['review_id']].copy()\n",
    "output_df_6['stars'] = [round(s, 0) for s in sentiment_scores]\n",
    "output_df_6[['review_id', 'stars']].to_csv('results_tournament_2/sentiment_analysis_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e4381",
   "metadata": {},
   "source": [
    "MAE público obtenido con análisis de sentimientos: **N/A**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
