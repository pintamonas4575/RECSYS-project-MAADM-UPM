{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8eeafe4",
   "metadata": {},
   "source": [
    "# **Filtrado basado en contenido**\n",
    "\n",
    "Álvaro Fraile, Jaime Álvarez, Alejandro Mendoza\n",
    "\n",
    "https://www.kaggle.com/competitions/recsys-filtrado-basado-en-contenido-2425"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e441b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f10d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1edd6f",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b92c7",
   "metadata": {},
   "source": [
    "### Negocios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27bef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negocios initial memory usage: 33.01 MB\n",
      "Negocios final memory usage: 19.96 MB\n",
      "***** Preprocesamiento negocios: 0 minutos y 0 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "negocios_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/negocios.csv')\n",
    "initial_memory = negocios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "negocios_df.drop(columns=['address', 'postal_code', 'is_open', 'hours'], inplace=True)\n",
    "for col in negocios_df.select_dtypes(include=['object']):\n",
    "    negocios_df[col] = negocios_df[col].astype(\"category\")\n",
    "negocios_df['latitude'] = negocios_df['latitude'].astype('float16')\n",
    "negocios_df['longitude'] = negocios_df['longitude'].astype('float16')\n",
    "negocios_df['stars'] = negocios_df['stars'].astype('float16')\n",
    "negocios_df['review_count'] = negocios_df['review_count'].astype('int16')\n",
    "\n",
    "final_memory = negocios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento negocios: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8697ab",
   "metadata": {},
   "source": [
    "### Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7456d25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrai\\AppData\\Local\\Temp\\ipykernel_3648\\57253039.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  usuarios_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/usuarios.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negocios initial memory usage: 1285.09 MB\n",
      "Negocios final memory usage: 1123.49 MB\n",
      "***** Preprocesamiento usuarios: 0 minutos y 8 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "usuarios_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/usuarios.csv')\n",
    "initial_memory = usuarios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "usuarios_df.drop(columns=['elite', 'yelping_since'], inplace=True)\n",
    "usuarios_df['user_id'] = usuarios_df['user_id'].astype('string')\n",
    "usuarios_df['name'] = usuarios_df['name'].astype('category')\n",
    "usuarios_df['friends'] = usuarios_df['friends'].astype('category')\n",
    "usuarios_df['useful'] = usuarios_df['useful'].astype('int32')\n",
    "usuarios_df['funny'] = usuarios_df['funny'].astype('int32')\n",
    "usuarios_df['cool'] = usuarios_df['cool'].astype('int32')\n",
    "usuarios_df['average_stars'] = usuarios_df['average_stars'].astype('float16')\n",
    "for col in usuarios_df.select_dtypes(include=['int64']):\n",
    "    usuarios_df[col] = usuarios_df[col].astype('uint16')\n",
    "\n",
    "final_memory = usuarios_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Negocios final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento usuarios: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ecacb",
   "metadata": {},
   "source": [
    "### Train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e23a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews initial memory usage: 896.88 MB\n",
      "Train reviews final memory usage: 746.68 MB\n",
      "***** Preprocesamiento train reviews: 0 minutos y 6 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_reviews_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/train_reviews.csv')\n",
    "initial_memory = train_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "train_reviews_df.drop(columns=['date'], inplace=True)\n",
    "train_reviews_df['review_id'] = train_reviews_df['review_id'].astype('string')\n",
    "train_reviews_df['user_id'] = train_reviews_df['user_id'].astype('category')\n",
    "train_reviews_df['business_id'] = train_reviews_df['business_id'].astype('category')\n",
    "train_reviews_df['text'] = train_reviews_df['text'].astype('string')\n",
    "\n",
    "final_memory = train_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento train reviews: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e232282",
   "metadata": {},
   "source": [
    "### Test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddbb3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews initial memory usage: 381.26 MB\n",
      "Train reviews final memory usage: 323.57 MB\n",
      "***** Preprocesamiento test reviews: 0 minutos y 2 segundos *****\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "test_reviews_df = pd.read_csv('data/recsys-filtrado-basado-en-contenido-24-25/test_reviews.csv')\n",
    "initial_memory = test_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews initial memory usage: {initial_memory:.2f} MB')\n",
    "\n",
    "test_reviews_df.drop(columns=['date'], inplace=True)\n",
    "test_reviews_df['review_id'] = test_reviews_df['review_id'].astype('string')\n",
    "test_reviews_df['user_id'] = test_reviews_df['user_id'].astype('category')\n",
    "test_reviews_df['business_id'] = test_reviews_df['business_id'].astype('category')\n",
    "test_reviews_df['text'] = test_reviews_df['text'].astype('string')\n",
    "\n",
    "final_memory = test_reviews_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Train reviews final memory usage: {final_memory:.2f} MB')\n",
    "\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Preprocesamiento test reviews: {int(minutos)} minutos y {int(segundos)} segundos *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba6830",
   "metadata": {},
   "source": [
    "## **Submission DataFrame skeleton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bfafd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = test_reviews_df[['review_id']].copy() # dataframe with review_id column\n",
    "global_avg = train_reviews_df['stars'].mean() # global average rating value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb7024",
   "metadata": {},
   "source": [
    "## Aproximación 1 - Media del negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb307f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of avg_ratings: 30064\n",
      "Length of negocios_df: 30069\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>avg_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--ARBQr1WMsTWiwOKOj-FQ</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--LC8cIrALInl2vyo701tg</td>\n",
       "      <td>4.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--N9yp3ZWqQIm7DqKRvorg</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--S43ruInmIsGrnnkmavRw</td>\n",
       "      <td>3.380952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  avg_stars\n",
       "0  --7PUidqRWpRSpXebiyxTg   1.900000\n",
       "1  --ARBQr1WMsTWiwOKOj-FQ   4.666667\n",
       "2  --LC8cIrALInl2vyo701tg   4.600000\n",
       "3  --N9yp3ZWqQIm7DqKRvorg   2.500000\n",
       "4  --S43ruInmIsGrnnkmavRw   3.380952"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average rating for each business\n",
    "avg_ratings = train_reviews_df.groupby('business_id', observed=True)['stars'].mean().reset_index()\n",
    "avg_ratings.columns = ['business_id', 'avg_stars']\n",
    "print(\"Length of avg_ratings:\", len(avg_ratings))\n",
    "print(\"Length of negocios_df:\", len(negocios_df))\n",
    "avg_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e96194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [01:19<00:00, 5246.84it/s]\n"
     ]
    }
   ],
   "source": [
    "output_df_1 = output_df.copy()\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df_1.loc[index, 'stars'] = (\n",
    "        avg_ratings.loc[avg_ratings['business_id'] == review['business_id'], 'avg_stars'].values[0]\n",
    "        if review['business_id'] in avg_ratings['business_id'].values else global_avg\n",
    "    )\n",
    "\n",
    "output_df_1.to_csv('results_tournament_2/submission_business_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd2bc3",
   "metadata": {},
   "source": [
    "MAE Público obtenido: \n",
    "* Usando 3 como default: **1.0433**\n",
    "* Usando media global como default: **1.0433**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24a2b4",
   "metadata": {},
   "source": [
    "## Aproximación 1.1 - Con redondeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780e60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_2 = output_df_1.copy() \n",
    "output_df_2['stars'] = output_df_2['stars'].round()\n",
    "output_df_2.to_csv('results_tournament_2/submission_business_avg_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96f8b2",
   "metadata": {},
   "source": [
    "MAE Público obtenido con redondeo: **1.0286**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0788d1",
   "metadata": {},
   "source": [
    "## Aproximación 2 - Embeddings con TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee5e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizando con TF-IDF...\n",
      "***** TF-IDF vectorization: 0 minutes 34 seconds *****\n",
      "Calculando similitud...\n",
      "***** Cosine similarity: 2 minutes 24 seconds *****\n",
      "Creando diccionario de ratings por usuario...\n",
      "***** User ratings dictionary: 4 minutes 5 seconds *****\n",
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:54<00:00, 7542.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Total time: 8 minutes 0 seconds *****\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "# Paso 1: Agrupar reviews por negocio\n",
    "business_reviews = train_reviews_df.groupby('business_id', observed=True)['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Paso 2: Vectorizar con TF-IDF\n",
    "print(\"Vectorizando con TF-IDF...\")\n",
    "start = time.time()\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(business_reviews['text'])\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** TF-IDF vectorization: {int(minutos)} minutes {int(segundos)} seconds *****\")\n",
    "\n",
    "# Paso 3: Calcular similitud entre negocios\n",
    "print(\"Calculando similitud...\")\n",
    "start = time.time()\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Cosine similarity: {int(minutos)} minutes {int(segundos)} seconds *****\")\n",
    "\n",
    "# Paso 4: Índice para acceder por business_id\n",
    "business_indices = pd.Series(business_reviews.index, index=business_reviews['business_id'])\n",
    "\n",
    "# Paso 5: Crear un diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "start = time.time()\n",
    "user_ratings = train_reviews_df.groupby('user_id', observed=True)\n",
    "user_ratings = user_ratings.apply(lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars'], include_groups=False).to_dict()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** User ratings dictionary: {int(minutos)} minutes {int(segundos)} seconds *****\")\n",
    "\n",
    "# Paso 6: Función para predecir rating\n",
    "def predict_rating(user_id: str, target_business_id: str) -> float:\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "\n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0:  # Considerar solo similares positivos\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg # No hay similitud con los ítems que ha valorado\n",
    "\n",
    "    # Promedio ponderado\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 7: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "output_df_3 = output_df.copy()\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df_3.loc[index, 'stars'] = predict_rating(review['user_id'], review['business_id'])\n",
    "\n",
    "total_minutes, total_seconds = divmod(time.time() - total_time, 60)\n",
    "print(f\"***** Total time: {int(total_minutes)} minutes {int(total_seconds)} seconds *****\")\n",
    "\n",
    "output_df_3['stars'] = output_df_3['stars'].round()\n",
    "output_df_3.to_csv('results_tournament_2/submission_tfidf_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00896ad5",
   "metadata": {},
   "source": [
    "MAE público obtenido con TFIDF: **1.1597**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e9e0e",
   "metadata": {},
   "source": [
    "## Aproximación 3 - Embeddings con Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e86aaad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Cargando modelo de sentence-transformers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\workspace\\RECSYS-project-MAADM-UPM\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aleja\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86aaad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Cargando modelo de sentence-transformers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Carga de modelo: 0 minutos 7 segundos *****\n",
      "Vectorizando con sentence-transformers...\n",
      "***** Vectorización: 1 minutos 23 segundos *****\n",
      "Calculando similitud...\n",
      "***** Cosine similarity: 0 minutos 1 segundos *****\n",
      "Creando diccionario de ratings por usuario...\n",
      "***** User ratings dictionary: 4 minutos 14 segundos *****\n",
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:57<00:00, 7207.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Total time: 6 minutes 45 seconds *****\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "\n",
    "# Paso 1: Agrupar reviews por negocio\n",
    "business_reviews = train_reviews_df.groupby('business_id', observed=True)['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Paso 2: Vectorizar con SentenceTransformer en GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "print(\"Cargando modelo de sentence-transformers...\")\n",
    "start = time.time()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Carga de modelo: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "print(\"Vectorizando con sentence-transformers...\")\n",
    "start = time.time()\n",
    "embeddings = model.encode(business_reviews['text'].tolist(), convert_to_tensor=True, device=device)\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Vectorización: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 3: Calcular similitud entre negocios\n",
    "print(\"Calculando similitud...\")\n",
    "start = time.time()\n",
    "cosine_sim = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()  # Para usarlo como matriz normal\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** Cosine similarity: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 4: Índice para acceder por business_id\n",
    "business_indices = pd.Series(business_reviews.index, index=business_reviews['business_id'])\n",
    "\n",
    "# Paso 5: Crear un diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "start = time.time()\n",
    "user_ratings = train_reviews_df.groupby('user_id', observed=True)\n",
    "user_ratings = user_ratings.apply(lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars'], include_groups=False).to_dict()\n",
    "minutos, segundos = divmod(time.time() - start, 60)\n",
    "print(f\"***** User ratings dictionary: {int(minutos)} minutos {int(segundos)} segundos *****\")\n",
    "\n",
    "# Paso 6: Función para predecir rating\n",
    "def predict_rating(user_id: str, target_business_id: str) -> float:\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0:  # Considerar solo similares positivos\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg  # No hay similitud con los ítems que ha valorado\n",
    "\n",
    "    # Promedio ponderado\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 7: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "output_df_4 = output_df.copy()\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df_4.loc[index, 'stars'] = predict_rating(review['user_id'], review['business_id'])\n",
    "\n",
    "total_minutes, total_seconds = divmod(time.time() - total_time, 60)\n",
    "print(f\"***** Total time: {int(total_minutes)} minutes {int(total_seconds)} seconds *****\")\n",
    "\n",
    "output_df_4['stars'] = output_df_4['stars'].round()\n",
    "output_df_4.to_csv('results_tournament_2/submission_tfidf_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82faf36f",
   "metadata": {},
   "source": [
    "MAE publico obtenido: 1.1594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b48f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Cargando modelo de sentence-transformers...\n",
      "Vectorizando cada review individual...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1891/1891 [07:55<00:00,  3.97it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m review_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m     10\u001b[0m     train_reviews_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     11\u001b[0m     convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Paso 2: Asociar embeddings a cada review\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrain_reviews_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(review_embeddings)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Paso 3: Agrupar por negocio y sacar el embedding medio\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculando embedding medio por negocio...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:5267\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m   5266\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m-> 5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n\u001b[0;32m   5275\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   5276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting an Index with object dtype into a DataFrame will stop \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferring another dtype in a future version. Cast the Index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5280\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   5281\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\pandas\\core\\construction.py:654\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    656\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[1;32mc:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:130\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    127\u001b[0m arr: ArrayLike\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mrange\u001b[39m)):\n\u001b[1;32m--> 130\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_1d_object_array_from_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# The caller is responsible for ensuring that we have np.ndarray\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m#  or ExtensionArray here.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     arr \u001b[38;5;241m=\u001b[39m values\n",
      "File \u001b[1;32mc:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1600\u001b[0m, in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m# numpy will try to interpret nested lists as further dimensions, hence\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;66;03m# making a 1D array that contains list-likes is a bit tricky:\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1600\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m values\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\afrai\\Documents\\MAADM\\RECSYS\\RECSYS-project-MAADM-UPM\\.venv\\lib\\site-packages\\torch\\_tensor.py:1196\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# Paso 1: Vectorizar cada review individual\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "print(\"Cargando modelo de sentence-transformers...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "print(\"Vectorizando cada review individual...\")\n",
    "review_embeddings = model.encode(\n",
    "    train_reviews_df['text'].tolist(),\n",
    "    convert_to_tensor=True,\n",
    "    device=device,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=512\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5cc7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando embedding medio por negocio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30064/30064 [00:02<00:00, 11034.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de embeddings...\n",
      "Calculando similitud coseno...\n",
      "Creando diccionario de ratings por usuario...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrai\\AppData\\Local\\Temp\\ipykernel_10640\\611155183.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_ratings = train_reviews_df.groupby('user_id').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:47<00:00, 8710.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Asociar embeddings a cada review\n",
    "train_reviews_df['embedding'] = list(review_embeddings.cpu())\n",
    "\n",
    "# Paso 3: Agrupar por negocio y sacar el embedding medio\n",
    "print(\"Calculando embedding medio por negocio...\")\n",
    "business_embeddings = {}\n",
    "for business_id, group in tqdm(train_reviews_df.groupby('business_id')):\n",
    "    embs = torch.stack(group['embedding'].tolist())\n",
    "    business_embeddings[business_id] = embs.mean(dim=0)\n",
    "\n",
    "# Paso 4: Crear lista de embeddings en orden\n",
    "print(\"Creando matriz de embeddings...\")\n",
    "business_ids = list(business_embeddings.keys())\n",
    "embedding_matrix = torch.stack([business_embeddings[b_id] for b_id in business_ids])\n",
    "\n",
    "# Paso 5: Calcular similitud coseno entre negocios\n",
    "print(\"Calculando similitud coseno...\")\n",
    "cosine_sim = util.pytorch_cos_sim(embedding_matrix, embedding_matrix).cpu().numpy()\n",
    "\n",
    "# Paso 6: Índice para acceder por business_id\n",
    "business_indices = pd.Series(range(len(business_ids)), index=business_ids)\n",
    "\n",
    "# Paso 7: Crear diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "user_ratings = train_reviews_df.groupby('user_id').apply(\n",
    "    lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars']\n",
    ").to_dict()\n",
    "\n",
    "# Paso 8: Función para predecir rating\n",
    "def predict_rating(user_id, target_business_id):\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "    \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0.8:\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg\n",
    "\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 9: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df.loc[index, 'stars'] = predict_rating(review.user_id, review.business_id)\n",
    "\n",
    "# Paso 10: Limpiar NaNs y redondear\n",
    "output_df['stars'] = output_df['stars'].fillna(global_avg).round()\n",
    "output_df.to_csv('./results/submission_sentence_transformers_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c2041d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999  0.4427101  0.29980582 ... 0.6780672  0.6563667  0.59870124]\n",
      " [0.4427101  1.0000001  0.44541562 ... 0.24072593 0.3650766  0.31967145]\n",
      " [0.29980582 0.44541562 0.9999999  ... 0.1681978  0.25902703 0.21196766]\n",
      " ...\n",
      " [0.6780672  0.24072593 0.1681978  ... 1.0000002  0.5487293  0.6100424 ]\n",
      " [0.6563667  0.3650766  0.25902703 ... 0.5487293  0.9999999  0.52822644]\n",
      " [0.59870124 0.31967145 0.21196766 ... 0.6100424  0.52822644 0.99999964]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d98a61",
   "metadata": {},
   "source": [
    "MAE publico: 1.1614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b519ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:45<00:00, 9182.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Paso 8: Función para predecir rating\n",
    "def predict_rating(user_id, target_business_id):\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "    \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = cosine_sim[target_idx, rated_idx]\n",
    "            if sim > 0.8:\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg\n",
    "\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n",
    "# Paso 9: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df.loc[index, 'stars'] = predict_rating(review.user_id, review.business_id)\n",
    "\n",
    "# Paso 10: Limpiar NaNs y redondear\n",
    "output_df['stars'] = output_df['stars'].fillna(global_avg).round()\n",
    "output_df.to_csv('./results/submission_sentence_transformers_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando distancia euclidiana...\n",
      "Creando diccionario de ratings por usuario...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrai\\AppData\\Local\\Temp\\ipykernel_10640\\1767733142.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_ratings = train_reviews_df.groupby('user_id').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:45<00:00, 9091.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Euclidean distance instead of cosine similarity\n",
    "print(\"Calculando distancia euclidiana...\")\n",
    "euclidean_dist = torch.cdist(embedding_matrix, embedding_matrix, p=2).cpu().numpy()\n",
    "\n",
    "# Convert Euclidean distance to similarity (1 / (1 + distance))\n",
    "euclidean_sim = 1 / (1 + euclidean_dist)\n",
    "\n",
    "# Paso 6: Índice para acceder por business_id\n",
    "business_indices = pd.Series(range(len(business_ids)), index=business_ids)\n",
    "\n",
    "# Paso 7: Crear diccionario de ratings por usuario\n",
    "print(\"Creando diccionario de ratings por usuario...\")\n",
    "user_ratings = train_reviews_df.groupby('user_id').apply(\n",
    "    lambda x: x[['business_id', 'stars']].set_index('business_id').to_dict()['stars']\n",
    ").to_dict()\n",
    "\n",
    "# Paso 8: Función para predecir rating\n",
    "def predict_rating(user_id, target_business_id):\n",
    "    if user_id not in user_ratings or target_business_id not in business_indices:\n",
    "        return global_avg\n",
    "    \n",
    "    rated_items = user_ratings[user_id]\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    target_idx = business_indices[target_business_id]\n",
    "\n",
    "    for rated_business_id, rating in rated_items.items():\n",
    "        if rated_business_id in business_indices:\n",
    "            rated_idx = business_indices[rated_business_id]\n",
    "            sim = euclidean_sim[target_idx, rated_idx]\n",
    "            if sim > 0.8:\n",
    "                similarities.append(sim)\n",
    "                ratings.append(rating)\n",
    "\n",
    "    if not similarities:\n",
    "        return global_avg\n",
    "\n",
    "    weighted_avg = np.dot(similarities, ratings) / np.sum(similarities)\n",
    "    return weighted_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc295474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414765/414765 [00:44<00:00, 9239.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Paso 9: Predecir ratings\n",
    "print(\"Prediciendo ratings...\")\n",
    "for index, review in tqdm(test_reviews_df.iterrows(), total=len(test_reviews_df)):\n",
    "    output_df.loc[index, 'stars'] = predict_rating(review.user_id, review.business_id)\n",
    "\n",
    "# Paso 10: Limpiar NaNs y redondear\n",
    "output_df['stars'] = output_df['stars'].fillna(global_avg).round()\n",
    "output_df.to_csv('./results/submission_sentence_transformers_rounded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18c5da",
   "metadata": {},
   "source": [
    "### Aproximación 4 - Analisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28174eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando sentimiento por batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando batches: 100%|██████████| 811/811 [30:33<00:00,  2.26s/it] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.eval()\n",
    "\n",
    "# Asegurar uso de GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Preprocesar textos\n",
    "MAX_LEN = 512\n",
    "texts = test_reviews_df['text'].apply(lambda x: x[:MAX_LEN]).tolist()\n",
    "\n",
    "# Función para procesar en lotes\n",
    "def get_sentiment_scores_batched(texts, batch_size=256):\n",
    "    scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Procesando batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        encoded_batch = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "        encoded_batch = {k: v.to(device) for k, v in encoded_batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_batch)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            probs = softmax(logits, axis=1)\n",
    "\n",
    "            # Map: negative → 0, neutral → 2.5, positive → 5\n",
    "            batch_scores = probs[:, 0]*0 + probs[:, 1]*2.5 + probs[:, 2]*5\n",
    "            scores.extend(batch_scores.tolist())\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Calcular los scores en batch\n",
    "print(\"Calculando sentimiento por batches...\")\n",
    "sentiment_scores = get_sentiment_scores_batched(texts, batch_size=512)\n",
    "\n",
    "# Guardar en el DataFrame\n",
    "test_reviews_df['stars'] = [round(s, 0) for s in sentiment_scores]\n",
    "\n",
    "# Exportar resultado\n",
    "test_reviews_df[['review_id', 'stars']].to_csv('./results_tournament_2/sentiment_analysis_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
